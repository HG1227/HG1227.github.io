---
layout: post
title:  机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法
date:   2019-12-25
categories: math 
tags: Matrix
---
* content
{:toc}
标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。

标量对向量或矩阵求导这两种情况，以分母布局为默认布局。向量对向量求导，以分子布局为默认布局。



# **1. 用定义法求解标量对向量求导**

标量对向量求导，严格来说是实值函数对向量的求导。即定义实值函数 $f: R^{n} \to R$  ,自变量 $\mathbf{x}$ 是n维向量，而输出 $y$ 是标量。对于一个给定的实值函数，如何求解 $\frac{\partial y}{\partial \mathbf{x}}$  呢？

 

首先我们想到的是基于矩阵求导的定义来做，由于所谓标量对向量的求导，其实就是标量对向量里的每个分量分别求导，最后把求导的结果排列在一起，按一个向量表示而已。那么我们可以将实值函数对向量的每一个分量来求导，最后找到规律，得到求导的结果向量。



首先我们来看一个简单的例子：$y=\mathbf{a}^T\mathbf{x}$  ,求解 $\frac{\partial \mathbf{a}^T\mathbf{x}}{\partial \mathbf{x}}$ 

根据定义，我们先对 $\mathbf{x}$ 的第 $i$ 个分量进行求导，这是一个标量对标量的求导，如下：


$$
\begin{equation}
\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial x_{i}}=\frac{\partial \sum_{j=1}^{n} a_{j} x_{j}}{\partial x_{i}}=\frac{\partial a_{i} x_{i}}{\partial x_{i}}=a_{i}
\end{equation}
$$


可见，对向量的第 $i$  个分量的求导结果就等于向量 $\mathbf{a}$ 的第i个分量。由于我们是分母布局，最后所有求导结果的分量组成的是一个n维向量。那么其实就是向量 $\mathbf{a}$ 。也就是说：


$$
\frac{\partial \mathbf{a}^T\mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}
$$


同样的思路，我们也可以直接得到：


$$
\frac{\partial \mathbf{x}^T\mathbf{a}}{\partial \mathbf{x}} = \mathbf{a}
$$


再来看一个复杂一点点的例子：$y=\mathbf{x}^T\mathbf{A}\mathbf{x}$  ,求解 $\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}$ 



我们对 $\mathbf{x}$ 的第k个分量进行求导如下：


$$
\begin{equation}
\frac{\partial \mathbf{x}^{T} \mathbf{A} \mathbf{x}}{\partial x_{k}}=\frac{\partial \sum_{i=1}^{n} \sum_{j=1}^{n} x_{i} A_{i j} x_{j}}{\partial x_{k}}=\sum_{i=1}^{n} A_{i k} x_{i}+\sum_{j=1}^{n} A_{k j} x_{j}
\end{equation}
$$


这个第 k 个分量的求导结果稍微复杂些了，仔细观察一下，第一部分是矩阵 $\mathbf{A}$ 的第 k 列转置后和 $x$ 相乘得到，第二部分是矩阵 $\mathbf{A}$ 的第 k 行和 $x$ 相乘得到，排列好就是:


$$
\begin{equation}
\frac{\partial \mathbf{x}^{T} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}}=\mathbf{A}^{T} \mathbf{x}+\mathbf{A} \mathbf{x}
\end{equation}
$$


从上面可以看出，定义法求导对于简单的实值函数是很容易的，但是复杂的实值函数就算求出了任意一个分量的导数，要排列出最终的求导结果还挺麻烦的，因此我们需要找到其他的简便一些的方法来整体求导，而不是每次都先去针对任意一个分量，再进行排列。



# **2. 标量对向量求导的一些基本法则**

在我们寻找一些简单的方法前，我们简单看下标量对向量求导的一些基本法则，这些法则和标量对标量求导的过程类似。

1. 常量对向量的求导结果为0。

2. 线性法则：如果 $f,g$  都是实值函数，$c_1,c_2$  为常数，则：  


$$
\begin{equation}
   \frac{\partial\left(c_{1} f(\mathbf{x})+c_{2} g(\mathbf{x})\right.}{\partial \mathbf{x}}=c_{1} \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}+c_{2} \frac{\partial g(\mathbf{x})}{\partial \mathbf{x}}
   \end{equation}
$$

3. 乘法法则：如果  $f,g$  都是实值函数，则：
   
   
   $$
   \begin{equation}
   \frac{\partial f(\mathbf{x}) g(\mathbf{x})}{\partial \mathbf{x}}=f(\mathbf{x}) \frac{\partial g(\mathbf{x})}{\partial \mathbf{x}}+\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}} g(\mathbf{x})
   \end{equation}
   $$
   
   要注意的是如果不是实值函数，则不能这么使用乘法法则。 
   
4. 除法法则：如果  $f,g$  都是实值函数，且  $g(\mathbf{x}) \neq 0$  ，则：

   $$
   \begin{equation}
   \frac{\partial f(\mathbf{x}) / g(\mathbf{x})}{\partial \mathbf{x}}=\frac{1}{g^{2}(\mathbf{x})}\left(g(\mathbf{x}) \frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}-f(\mathbf{x}) \frac{\partial g(\mathbf{x})}{\partial \mathbf{x}}\right)
   \end{equation}
   $$

   











