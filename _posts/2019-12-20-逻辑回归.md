---
layout: post
title:  逻辑回归 (Logistic Regression )
date:   2019-12-20
categories: Machine-learning
tags: Logistic-Regression    
---
* content
{:toc}
逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。虽然它名字里面有“回归”两个字，却不是一个回归算法。那为什么有“回归”这个误导性的词呢 ？个人认为，虽然逻辑回归是分类模型，但是它的原理里面却残留着回归模型的影子，









## **1. 从线性回归到逻辑回归**

线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数 $θ$，满足 $\mathbf{Y = X\theta}$ 。此时我们的Y是连续的，所以是回归模型。如果我们想要Y是离散的话，怎么办呢？一个可以想到的办法是，我们对于这个Y再做一次函数转换，变为 $g(Y)$。如果我们令 $g(Y)$ 的值在某个实数区间的时候是类别 A，在另一个实数区间的时候是类别 B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。逻辑回归的出发点就是从这来的。



## **2. 二元逻辑回归的模型**

对线性回归的结果做一个在函数 g 上的转换，可以变化为逻辑回归。这个函数 g 在逻辑回归中我们一般取为 `sigmoid` 函数，形式如下：


$$
g(z) = \frac{1}{1+e^{-z}}
$$


<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20191220094844.jpg"/>
</center>

它有一个非常好的性质，即当z趋于正无穷时，$g(z)$ 趋于1，而当z趋于负无穷时，$g(z)$ 趋于0，这非常适合于我们的分类概率模型。另外，它还有一个很好的导数性质：


$$
g^{'}(z) = g(z)(1-g(z))
$$


这个通过函数对 $g(z)$ 求导很容易得到，后面我们会用到这个式子。

如果我们令 $g(z)$ 中的 $z$ 为：$z=xθ$，这样就得到了二元逻辑回归模型的一般形式：


$$
h_{\theta}(x) = \frac{1}{1+e^{-x\theta}}
$$


