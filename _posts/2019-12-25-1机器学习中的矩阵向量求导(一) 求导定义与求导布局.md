---
layout: post
title:  机器学习中的矩阵向量求导(一) 求导定义与求导布局
date:   2019-12-25
categories: math 
tags: Matrix
---
* content
{:toc}
机器学习中，矩阵向量求导的方法 。







## **1. 矩阵向量求导引入**

在高等数学里面，我们已经学过了标量对标量的求导，比如标量 $y$ 对标量 $x$ 的求导，可以表示为 $\frac{\partial y}{\partial x}$  .<br>

有些时候，我们会有一组标量 $y_i,i=1,2,...,m$  来对一个标量 $x$ 的求导,那么我们会得到一组标量求导的结果：


$$
\frac{\partial y_i}{\partial x}, i=1,2.,,,m
$$


如果我们把这组标量写成向量的形式，即得到维度为 m 的一个向量 $\mathbf{y}$ 对一个标量 $x$ 的求导，那么结果也是一个 m 维的向量：$\frac{\partial \mathbf{y}}{\partial x}$  <br>

所谓向量对标量的求导，其实就是向量里的每个分量分别对标量求导，最后把求导的结果排列在一起，按一个向量表示而已。类似的结论也存在于标量对向量的求导，向量对向量的求导，向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导等。

总而言之，所谓的向量矩阵求导本质上就是多元函数求导，仅仅是把把函数的自变量，因变量以及标量求导的结果排列成了向量矩阵的形式，方便表达与计算，更加简洁而已。



为了便于描述，求导的自变量用 $x$ 表示标量 , $\mathbf{x}$ 表示n维向量，$\mathbf{X}$ 表示 $m \times n$ 维度的矩阵，求导的因变量用 $y$ 表示标量，$\mathbf{y}$  表示m维向量，$\mathbf{Y}$ 表示 $p \times q$ 维度的矩阵。



## **2. 矩阵向量求导定义**

根据求导的自变量和因变量是标量，向量还是矩阵，我们有9种可能的矩阵求导定义，如下：

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20191230153701.png"/>
</center>



维度为 m 的一个向量 $\mathbf{y}$ 对一个标量 $x$ 的求导，那么结果也是一个 m 维的向量： $\frac{\partial \mathbf{y}}{\partial x}$ 。这是我们表格里面向量对标量求导的情况。这里有一个问题没有讲到，就是这个 m 维的求导结果排列成的m维向量到底应该是列向量还是行向量？



这个问题的答案是：行向量或者列向量皆可！毕竟我们求导的本质只是把标量求导的结果排列起来，至于是按行排列还是按列排列都是可以的。但是这样也有问题，在我们机器学习算法法优化过程中，如果行向量或者列向量随便写，那么结果就不唯一，乱套了。



为了解决这个问题，我们引入求导布局的概念。

<br>

## **3. 矩阵向量求导布局**

为了解决矩阵向量求导的结果不唯一，我们引入求导布局。最基本的求导布局有两个：**分子布局(numerator layout)和分母布局(denominator layout )**。 

<br>

对于分子布局来说，我们求导结果的维度以分子为主，比如对于我们上面对标量求导的例子，结果的维度和分子的维度是一致的。也就是说，如果向量 $\mathbf{y}$ 是一个 m 维的列向量，那么求导结果 $\frac{\partial \mathbf{y}}{\partial x}$ 也是一个 m 维列向量。如果向量 $\mathbf{y}$  是一个m维的行向量，那么求导结果 $\frac{\partial \mathbf{y}}{\partial x}$ 也是一个 m 维行向量。

<br>

 对于分母布局来说，我们求导结果的维度以分母为主，比如对于我们上面对标量求导的例子，如果向量 $\mathbf{y}$ 是一个 m 维的列向量，那么求导结果 $\frac{\partial \mathbf{y}}{\partial x}$ 是一个 m 维行向量。如果向量 $\mathbf{y}$  是一个 m 维的行向量，那么求导结果 $\frac{\partial \mathbf{y}}{\partial x}$ 是一个 m 维列向量。

可见，对于分子布局和分母布局的结果来说，两者相差一个转置。



比如 m 维列向量 $\mathbf{y}$ 对 n 维列向量 $\mathbf{x}$ 求导。它的求导结果在分子布局和分母布局各是什么呢？对于这2个向量求导，那么一共有 mn 个标量对标量的求导。求导的结果一般是排列为一个矩阵。如果是分子布局，则矩阵的第一个维度以分子为准，即结果是一个 $m \times n$ 的矩阵，如下：


$$
\begin{equation}
\frac{\partial \mathbf{y}}{\partial \mathbf{x}}=\left(\begin{array}{cccc}
{\frac{\partial y_{1}}{\partial x_{1}}} & {\frac{\partial y_{1}}{\partial x_{2}}} & {\cdots} & {\frac{\partial y_{1}}{\partial x_{n}}} \\
{\frac{\partial y_{2}}{\partial x_{1}}} & {\frac{\partial y_{2}}{\partial x_{2}}} & {\cdots} & {\frac{\partial y_{2}}{\partial x_{n}}} \\
{\vdots} & {\vdots} & {\ddots} & {\vdots} \\
{\frac{\partial y_{m}}{\partial x_{1}}} & {\frac{\partial y_{m}}{\partial x_{2}}} & {\cdots} & {\frac{\partial y_{m}}{\partial x_{n}}}
\end{array}\right)
\end{equation}
$$




上边这个按分子布局的向量对向量求导的结果矩阵，我们一般叫做**雅克比 (Jacobian)矩阵。**

<br>

如果是按分母布局，则求导的结果矩阵的第一维度会以分母为准，即结果是一个 $n \times m$ 的矩阵，如下：


$$
\begin{equation}
\frac{\partial \mathbf{y}}{\partial \mathbf{x}}=\left(\begin{array}{cccc}
{\frac{\partial y_{1}}{\partial x_{1}}} & {\frac{\partial y_{2}}{\partial x_{1}}} & {\ldots} & {\frac{\partial y_{m}}{\partial x_{1}}} \\
{\frac{\partial y_{1}}{\partial x_{2}}} & {\frac{\partial y_{2}}{\partial x_{2}}} & {\cdots} & {\frac{\partial y_{m}}{\partial x_{2}}} \\
{\vdots} & {\vdots} & {\ddots} & {\vdots} \\
{\frac{\partial y_{1}}{\partial x_{n}}} & {\frac{\partial y_{2}}{\partial x_{n}}} & {\cdots} & {\frac{\partial y_{m}}{\partial x_{n}}}
\end{array}\right)
\end{equation}
$$


上边这个按分母布局的向量对向量求导的结果矩阵，我们一般叫做**梯度矩阵。**

但是在机器学习算法原理的资料推导里，并没有看到说正在使用什么布局，也就是说布局被隐含了，这就需要自己去推演，比较麻烦。但是一般来说我们会使用一种叫混合布局的思路，即如果是向量或者矩阵对标量求导，则使用分子布局为准，如果是标量对向量或者矩阵求导，则以分母布局为准。对于向量对对向量求导，有些分歧.

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20191231112456.png"/>
</center>









## 参考

1. <a href="https://www.cnblogs.com/pinard/p/10750718.html" target="_blank">机器学习中的矩阵向量求导(一) 求导定义与求导布局</a> 