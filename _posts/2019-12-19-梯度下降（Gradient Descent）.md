---
layout: post
title:  梯度下降（Gradient Descent）
date:   2019-12-19
categories: Machine-learning
tags: Gradient-Descen    
---
* content
{:toc}
在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。









## **1. 梯度**

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $(∂f/∂x, ∂f/∂y)^T$ ,简称 $grad f(x,y)$ 或者$▽f(x,y)$ 。对于在点 $(x_0,y_0)$ 的具体梯度向量就是 $(∂f/∂x_0, ∂f/∂y_0)^T$ .或者 $▽f(x_0,y_0)$ ，如果是3个参数的向量梯度，就是 $(∂f/∂x, ∂f/∂y，∂f/∂z)^T$ ,以此类推 。



那么这个梯度向量求出来有什么意义呢？它的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数 $f(x,y)$ ,在点 $(x_0,y_0)$ ，沿着梯度向量的方向就是 $(∂f/∂x_0, ∂f/∂y_0)^T$  的方向是$f(x,y)$ 增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是$ -(∂f/∂x_0, ∂f/∂y_0)^T$ 的方向，梯度减少最快，也就是更加容易找到函数的最小值。



## **2. 梯度下降与梯度上升**

在机器学习算法中，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数 $f(θ)$ 的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数 $-f(θ)$ 的最大值，这时梯度上升法就派上用场了。



## **3. 梯度下降法算法详解**

### 3.1 梯度下降的直观解释

首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。



从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20191219101609.png"/>
</center>



### 3.2 梯度下降的相关概念

在详细了解梯度下降的算法之前，我们先看看相关的一些概念。

1. 步长（Learning rate）

   步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。

2. 特征（feature）

   指的是样本中输入部分，比如2个单特征的样本 $（x^{(0)},y^{(0)}）,（x^{(1)},y^{(1)}）$ ,则第一个样本特征为  $x^{(0)}$  , 第一个样本输出为 $y^{(0)}$  。

3. 假设函数（hypothesis function）
   

   在监督学习中，为了拟合输入样本，而使用的假设函数，记为 $ h_{\theta}(x)$ 。比如对于单个特征的m个样本 $（x^{(i)},y^{(i)}）(i=1,2,...m)$  , 可以采用拟合函数如下：$h_{\theta}(x) = \theta_0+\theta_1x$ .

4. 损失函数（loss function）

   为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于 m 个样本 $（x_i,y_i）(i=1,2,...m)$ , 采用线性回归，损失函数为：

   
   $$
   J(\theta_0, \theta_1) = \sum\limits_{i=1}^{m}(h_\theta(x_i) - y_i)^2
   $$
   
   其中 $x_i$ 表示第i个样本特征，$y_i$ 表示第 $i$ 个样本对应的输出，$h_θ(x_i)$ 为假设函数。  