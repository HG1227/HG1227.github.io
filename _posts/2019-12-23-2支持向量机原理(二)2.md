---
layout: post
title:  持向量机原理(二) 线性支持向量机的软间隔最大化模型
date:   2019-12-21
categories: Machine-learning
tags: SVM    ML
---
* content
{:toc}
在[支持向量机原理(一) 线性支持向量机](http://www.cnblogs.com/pinard/p/6097604.html)中，我们对线性可分SVM的模型和损失函数优化做了总结。有时候不能线性可分的原因是线性数据集里面多了少量的异常点，由于这些异常点导致了数据集不能线性可分，本篇就对线性支持向量机如何处理这些异常点的原理方法做一个总结。









# **1. 线性分类SVM面临的问题**

有时候本来数据的确是可分的，也就是说可以用 线性分类SVM的学习方法来求解，但是却因为混入了异常点，导致不能线性可分，比如下图，本来数据是可以按下面的实线来做超平面分离的，可以由于一个橙色和一个蓝色的异常点导致我们没法按照上一篇[线性支持向量机](http://www.cnblogs.com/pinard/p/6097604.html)中的方法来分类。

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20191225135319.png"/>
</center>

另外一种情况没有这么糟糕到不可分，但是会严重影响我们模型的泛化预测效果，比如下图，本来如果我们不考虑异常点，SVM的超平面应该是下图中的红色线所示，但是由于有一个蓝色的异常点，导致我们学习到的超平面是下图中的粗虚线所示，这样会严重影响我们的分类模型预测效果。

<center>
    <img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20191225135435.png"/>
</center>

如何解决这些问题呢？SVM引入了软间隔最大化的方法来解决。

<br>

# **2. 线性分类SVM的软间隔最大化**

所谓的软间隔，是相对于硬间隔说的，我们可以认为上一篇线性分类SVM的学习方法属于硬间隔最大化。

回顾下硬间隔最大化的条件：



$$
\begin{equation}
\begin{aligned}
\min &\frac{1}{2}\|w\|_{2}^{2} \\\text { s.t }\quad &y_{i}\left(w^{T} x_{i}+b\right) \geq 1\qquad(i=1,2, \dots m)
\end{aligned}
\end{equation}
$$





SVM对训练集里面的每个样本 $(x_i,y_i)$ 引入了一个松弛变量 $\xi_i \geq 0$  ,使函数间隔加上松弛变量大于等于1，也就是说：


$$
y_i(w\bullet x_i +b) \geq 1- \xi_i
$$


对比硬间隔最大化，可以看到我们对样本到超平面的函数距离的要求放松了，之前是一定要大于等于1，现在只需要加上一个大于等于0的松弛变量能大于等于1就可以了。当然，松弛变量不能白加，这是有成本的，每一个松弛变量  $\xi_i $ , 对应了一个代价  $\xi_i $ , 这个就得到了我们的软间隔最大化的SVM学习条件如下：


$$
min\;\; \frac{1}{2}||w||_2^2 +C\sum\limits_{i=1}^{m}\xi_i
$$

$$
s.t.  \;\; y_i(w^Tx_i + b)  \geq 1 - \xi_i \;\;(i =1,2,...m)
$$

$$
\xi_i \geq 0 \;\;(i =1,2,...m)
$$



这里, $C>0$  为惩罚参数，可以理解为我们一般回归和分类问题正则化时候的参数。$C$ 越大，对误分类的惩罚越大，$C$ 越小，对误分类的惩罚越小。



也就是说，我们希望 $\frac{1}{2}\| \|w\|\|_2^2$  尽量小，误分类的点尽可能的少。C是协调两者关系的正则化惩罚系数。在实际应用中，需要调参来选择。

这个目标函数的优化和上一篇的线性可分SVM的优化方式类似，我们下面就来看看怎么对线性分类SVM的软间隔最大化来进行学习优化。

<br>

# **3. 线性分类SVM的软间隔最大化目标函数的优化**

和线性可分SVM的优化方式类似，我们首先将软间隔最大化的约束问题用拉格朗日函数转化为无约束问题如下：

<br>
$$
L(w,b,\xi,\alpha,\mu) = \frac{1}{2}||w||_2^2 +C\sum\limits_{i=1}^{m}\xi_i - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] - \sum\limits_{i=1}^{m}\mu_i\xi_i
$$
<br>

其中 $\mu_i \geq 0, \alpha_i \geq 0$ ,均为拉格朗日系数。

也就是说，我们现在要优化的目标函数是：

<br>
$$
\underbrace{min}_{w,b,\xi}\; \underbrace{max}_{\alpha_i \geq 0, \mu_i \geq 0,} L(w,b,\alpha, \xi,\mu)
$$
<br>

这个优化目标也满足KKT条件，也就是说，我们可以通过拉格朗日对偶将我们的优化问题转化为等价的对偶问题来求解如下：

<br>
$$
\underbrace{max}_{\alpha_i \geq 0, \mu_i \geq 0,} \; \underbrace{min}_{w,b,\xi}\; L(w,b,\alpha, \xi,\mu)
$$
<br>


















































# 参考

1. <a href="https://www.cnblogs.com/pinard/p/6100722.html" target='_blank'> 支持向量机原理(二) 线性支持向量机的软间隔最大化模型</a>  
2. <a href="https://github.com/fengdu78/lihang-code/blob/master/第07章 支持向量机/7.support-vector-machine.ipynb" target='_blank'> 第7章 支持向量机</a> 
3. <a href="https://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html" target='_blank'>支持向量机SVM</a>  