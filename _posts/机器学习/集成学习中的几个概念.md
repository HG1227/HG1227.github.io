集成学习



集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。根据个体学习器的生成方式，目前集成学习的方法大致分为两类，即个体学习器之间存在强依赖关系，必须串行生成的序列化方法；另一类就是个体学习器之间不存在强依赖关系、可同时生成的并行化方法。前者的代表是Boosting，后者的代表室Bagging和随机森林。

## 集成学习中的几个概念

1、个体学习器：集成学习的一般结构都是先产生一组个体学习器（individual learner），在用某种策略将他们结合起来，个体学习器通常由一个现有的学习算法从训练数据中产生。

2、基学习器：如果集成中只包含同种类型的个体学习器，例如决策树集成中全都是决策树，这样的集成是‘同质’（homogeneous）的，**同质集成**中的个体学习器又称‘基学习器’，相应的学习算法又称基学习算法。

3、组件学习器：集成也可以是包含不同类型的个体学习器，例如决策树和神经网络，这样的集成是‘异质’（heterogenous）的，**异质集成**中的个体学习器称为组件学习器或者直接称为个体学习器。



从下图，我们可以对集成学习的思想做一个概括。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。



<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624093746.png" alt="1042406-20161204191919974-1029671964" style="zoom: 33%;" /></center>

　也就是说，集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。

## Hard Voting

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101035.png" alt="Snipaste_2020-06-23_10-29-20 (1)" style="zoom:50%;" /></center>

## Soft Voting

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101057.png" alt="Snipaste_2020-06-23_10-31-36 (1)" style="zoom:50%;" /></center>



集成学习中的soft voting要求集合的每一个模型都能估计概率。

（1）逻辑回归，本身就是基于概率模型的。

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101118.jpg" alt="v2-76cb72830ed1ae75b51ab0127fdc75f2_720w" style="zoom: 33%;" /></center>

（2）KNN，k个近邻中数量最多的那个类的数量除以k就是概率。也可以考虑权值。

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101136.jpg" alt="v2-86e454c69ab5e4f6d816337a20207c7f_720w" style="zoom:33%;" /></center>

（3）决策树。叶子节点中那个类数量最大就是那个类，概率就是数量最大的那个类除以所有叶子节点的类。

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101153.jpg" alt="v2-536d9965f3a5731456bc8d12c74eafae_720w" style="zoom:33%;" /></center>

（4）SVC。SVC本身是没有考虑概率的，它是寻找一个margin的最大值。但是也是可以计算过，不过需要消耗大量的计算资源。

## 集成学习优势

1，个体学习器之间存在一定的差异性，这会导致分类边界不同，也就是说可能存在错误。那么将多个个体学习器合并后，就可以得到更加合理的边界，减少整体的错误率，实现更好的效果。

2，对于数据集过大或者过小的情况，可以分别进行划分和有放回的操作，产生不同的数据子集，然后使用数据子集训练不同的学习器，最终再合并成为一个强学习器；

3，如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，然后再进行模型的融合。

4，对于多个异构的特征集的时候，很难直接融合，那么可以考虑使用每个数据集构建一个分类模型，然后将多个模型融合。



## 常见的集成学习框架

　根据上述所说的单个学习器的产生过程不同，集成学习大致可以分为两类：串行，并行

　　**串行**：个体学习器们的产生依赖彼此，比如当前学习器的产生依赖上一个学习器的参数，所以最终将单个学习器们组合形式可以看做是串行序列化方法，代表是Boosting。

　　**并行**：个体学习器间不存在强依赖关系，可以并行化同时产生，代表是Bagging。

- **1，用于减少方差的Bagging**
- **2，用于减少偏差的Boosting**
- **3，用于提升预测结果的Stacking**

### 1、Bagging（bootstrap aggregating）

Bagging 也叫自举汇聚法（bootstrap aggregating）或者套袋法，指从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果 （说白了就是并行训练一堆分类器）。最典型的代表就是随机森林了。

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101218.png" alt="1226410-20190220195006412-3608833" style="zoom:67%;" /></center>

或者

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101241.png" alt="1226410-20200409192336409-149282323" style="zoom:67%;" /></center>

Bagging 是一种在原始数据集上通过**有放回抽样**重新选出K个新数据集来训练分类器的集成技术。它使用训练出来的分类器的集合来对新样本进行分类，然后用的多数投票或者对输出求均值的方法统计所有分类器的分类结果，结果最高的类别即为最终标签，此类算法可以有效降低 bias，并能够降低 variance。

#### Bagging的算法步骤

Bagging算法过程如下：

（A） 从原始样本集中抽取训练集。每轮从原始样本集中使用 Bootsraping的方法抽取 n 个训练样本（**在初始训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽取中**）。共进行 K 轮抽取，得到 K 个训练集（k 个训练集之间是相互独立的），其中初始训练集中约有63.2%的样本出现在采样集中。

（B）每次使用一个训练集得到一个模型，K 个训练集共得到 K 个模型（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或者回归方法，如决策树，感知器等）

（C）对于分类问题：将上步得到的K个模型**采用投票的方式**得到分类结果；

 对于回归问题：计算上述模型的**均值**作为最后的结果（所有模型的重要性相等）

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624101306.png" alt="1226410-20200410191821432-686912159" style="zoom:67%;" /></center>

####  **Bagging的优点：**

- 1，训练一个Bagging集成与直接使用基分类器算法训练一个学习器的复杂度同阶，说明Bagging是一个高效的集成学习算法
- 2，与标准的Adaboost算法只适用于二分类问题不同，Bagging能不经过修改用于多分类，回归等任务
- 3，由于每个基学习器只使用了 63.2%的数据，所以剩下的36.8%的数据可以用来做验证集来对泛化性能进行“包外估计”

### 2、Boosting模型

Boosting的思想源于1986年Valiant提出的PCA（probably Approximately Correct）学习模型。Valiant和Kearns提出了弱学习和强学习的概念。识别错误率小于1/2，也即准确率仅比随机猜测略高的学习算法称为弱学习算法；识别准确率很高并能在多项式时间内完成的学习算法称为强学习算法。同时，Valiant和Kearns首次提出了PAC学习模型中弱学习算法和强学习算法的等价性问题，即任意给定仅比随机猜测略好的弱学习算法就可以将其提升为强学习算法，而不必寻找很难获得的强学习算法。1990年，Schapire最先构造出一种多项式级的算法，对该问题做了肯定的证明，这就是最初的Boosting算法。一年后，Freund提出了一种效率更高的Boosting算法。但是这两种算法存在共同的实践上的缺陷，那就是都要求事先知道弱学习算法学习正确的下限，1995年，Freund和schap ire改进了Boosting算法，提出了AdaBoost（Adap tive Boosting）算法，该算法效率和Freund于1991年提出的Boosting算法几乎相同，但是不需要任何关于弱分类器的先验知识，因而更容易应用到实际问题当中。

　　Boosting模型的训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转换。对所有基模型预测的结果进行线性综合产生最终的预测结果。Boosting模型的典型代表：AdaBoost，XGBoost，其中AdaBoost会根据前一次的分类效果调整数据权重

#### Boosting模型的工作机制

　这一类算法的工作机制大致如下：**先从初始训练集练出一个集学习器，再根据即学习器的表现对训练样本进行调整，使得先前基学习器做错的样子在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值，最终将这T个基学习器进行加权结合。**

　　Boosting模型简单来说就是如果在某一个数据在这次分错了，那么在下一次我就会给他更大的权重，最终的结果就是：每个分类器根据自身的准确性来确定各自的权重，再合体。

<center><img src="https://raw.githubusercontent.com/HG1227/image/master/img_tuchuang/20200624142613.png" alt="1226410-20190220195434079-94624386" style="zoom:67%;" /></center>

#### Boosting模型的两个核心问题

　Boosting模型的主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。关于Boosting的两个核心问题：

**（1）在每轮如何改变训练数据的权值或者概率分布？**

　　通过提高哪些在前一轮被弱分类器分错样例的权值，减少前一轮分对样本的权值，来使得分类器对误分的数据有较好的效果

**（2）通过什么方式来组合弱分类器**

　　通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减少错误率较大的分类器的权值。而提升树通过拟合残差的方式逐步减少残差，将每一步生成的模型叠加得到最终模型。

　　对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易的多。Boosting方法就是从“弱学习器”出发，反复学习，得到一系列弱分类器，然后组合这些分类器，构成一个强分类器。



Boosting框架的代表算法——XGBoost算法，Adaboost算法

这里主要涉及两个权重的计算问题：

（1） 样本的权值

- 没有先验知识的情况下，初始的分布应为等概分布，样本数目为n，权值为1/n
- 每一次的迭代更新权值，提高分错样本的权重

（2） 弱学习器的权值

- 最后的强学习器是通过多个基学习器通过权值组合得到的
- 通过权值体现不同基学习器的影响，正确率高的基学习器权重高，实际上是分类误差的一个函数



## 结合策略

通过上面，我们对集成学习算法也有了大概的了解，简单来说，集成算法就是训练一堆基学习器，然后通过某种策略把各个基学习器的结果进行合成，从而得到集成学习器的结果。下面学习一下常用的结合策略。

### 平均法（Averaging）

对于数值型（连续）输出 $$h_{i}(x) \in R$$ 最常见的结合策略为平均法。

简单平均法（Sample averaging）


$$
\begin{equation}H(x)=\frac{1}{T} \sum_{i=1}^{T} h_{i}(x)\end{equation}
$$


加权平均法（weighted averaging）
$$
H(x)=\sum_{i=1}^{T} w_{i} h_{i}(x)
$$
　

其中 $ W_i $为权重，通常 $W_i $要求：


$$
\begin{equation}w_{i} \geq 0, \sum_{i=1}^{T} w_{i}=1\end{equation}
$$
　注意：加权平均法的权重一般从训练数据中学习而得，对规模比较大的集成学习来说，要学习的权重比较多，比较容易过拟合，因此加权平均法就不一定优于简单平均法。**一般而言，在个体学习器性能相差比较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法**。

### 投票法（Voting）

对于分类来说，学习器  $h_i(x)$ 将从类别集合中预测出一个类别标记，最常用的是投票法。投票法的基本思想就是组合不同的机器学习模型，通过投票或者平均的方法预测最后的标签。分类的机器学习算法输出有两种类型：一种是直接输出类标签，另外一种是输出类概率，使用前者进行投票叫做硬投票(Majority/Hard voting)，使用后者进行分类叫做软投票(Soft voting)。

——绝对多数投票法（majority/Hard voting）
$$
\begin{equation}H(x)=\left\{\begin{array}{ll}
c_{j}, & \text { if } \sum_{i=1}^{T} h_{i}^{j}(x)>0.5 \sum_{k=1}^{N} \sum_{i=1}^{T} h_{j}^{k}(x) \\
\text {reject,} & \text { other wise }
\end{array}\right.\end{equation}
$$
即如果某标记的投票过半数，则预计为该标记（直接输出标签）；否则拒绝预测。

——相对多数投票法（plurality voting）
$$
\begin{equation}H(x)=c_{a r g} \max _{j} \sum_{i=1}^{T} h_{i}^{j}(x)\end{equation}
$$
即预测为得票最多的标记，若同时出现多个票数最多，则任选其一（在算法中会将预测标签升序排列，选最上面的）。



——加权投票法（weighted voting）
$$
\begin{equation}H(x)=c_{a r g} \max _{j} \sum_{i=1}^{T} w_{i} h_{i}^{j}(x)\end{equation}
$$
与加权平均法类似。对分类器进行分配权重，这个权重不是我们自己分配的，是通过训练数据得到的。

###  学习法

上两节的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。

在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。

## 参考

- <a href="https://www.cnblogs.com/wj-1314/p/10917286.html" target="_blank">Python机器学习笔记：集成学习总结</a> 

