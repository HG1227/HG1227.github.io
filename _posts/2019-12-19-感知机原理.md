---
layout: post
title: 感知机原理
categories: Machine-learning
tags: perceptron 
---
* content
{:toc}
感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。









## **1. 感知机模型**

感知机的思想很简单，比如我们在一个平台上有很多的男孩女孩，感知机的模型就是尝试找到一条直线，能够把所有的男孩和女孩隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，**能够把所有的二元类别隔离开**。当然你会问，如果我们找不到这么一条直线的话怎么办？找不到的话那就意味着类别线性不可分，也就意味着感知机模型不适合你的数据的分类。**使用感知机一个最大的前提，就是数据是线性可分的。**这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。

**感知机**（perceptron）是二分类的线性分类模型，属于**监督学习算法**。



用数学的语言来说，如果我们有m个样本，每个样本对应于n维特征和一个二元类别输出，如下：


$$
(x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)
$$


我们的目标是找到这样一个超平面，即：


$$
\theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n} = 0
$$


让其中一种类别的样本都满足 $\theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n} > 0$ , 让另一种类别的样本都满足

 $\theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n} < 0$  , 从而得到线性可分。如果数据线性可分，这样的超平面一般都不是唯一的，也就是说感知机模型可以有多个解。



为了简化这个超平面的写法，我们增加一个特征 $x_0 = 1$  ，这样超平面为 $\sum\limits_{i=0}^{n}\theta_{i}x_{i} = 0$  进一步用向量来表示为：$\theta \bullet x = 0$  , 其中 $θ$ 为 $(n+1)\times1$ 的向量，$x$ 为 $(n+1)\times 1$ 的向量, ∙∙为内积，后面我们都用向量来表示超平面。

 

而感知机的模型可以定义为：$y = sign(\theta \bullet x)$  其中：


$$
sign(x)= \begin{cases} -1& {x<0}\\ 1& {x\geq 0} \end{cases}
$$


## **2. 感知机模型损失函数**

为了后面便于定义损失函数，我们将满足 $\theta \bullet x > 0$  的样本类别输出值取为1，满足 $\theta \bullet x < 0$ 的样本类别输出值取为-1，  这样取y的值有一个好处，就是方便定义损失函数。因为正确分类的样本满足 $y\theta \bullet x > 0$ ，而错误分类的样本满足 $y\theta \bullet x < 0$  , 我们损失函数的优化目标，就是期望使误分类的所有样本，到超平面的距离之和最小。

由于 $y\theta \bullet x < 0$  ，所以对于每一个误分类的样本 $i $ ，到超平面的距离是


$$
- y^{(i)}\theta \bullet x^{(i)}\big / \|\theta\|_2
$$


其中 $ \|\theta\|_2  $ 为L2范数。

