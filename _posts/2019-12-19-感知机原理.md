---
layout: post
title: 感知机原理
categories: Machine-learning
tags: perceptron 
---
* content
{:toc}
感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。









## **1. 感知机模型**

感知机的思想很简单，比如我们在一个平台上有很多的男孩女孩，感知机的模型就是尝试找到一条直线，能够把所有的男孩和女孩隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，**能够把所有的二元类别隔离开**。当然你会问，如果我们找不到这么一条直线的话怎么办？找不到的话那就意味着类别线性不可分，也就意味着感知机模型不适合你的数据的分类。**使用感知机一个最大的前提，就是数据是线性可分的。**这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。

**感知机**（perceptron）是二分类的线性分类模型，属于**监督学习算法**。



用数学的语言来说，如果我们有m个样本，每个样本对应于n维特征和一个二元类别输出，如下：


$$
(x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)
$$


我们的目标是找到这样一个超平面，即：


$$
\theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n} = 0
$$


让其中一种类别的样本都满足 $\theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n} > 0$ , 让另一种类别的样本都满足

 $\theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n} < 0$  , 从而得到线性可分。如果数据线性可分，这样的超平面一般都不是唯一的，也就是说感知机模型可以有多个解。



为了简化这个超平面的写法，我们增加一个特征 $x_0 = 1$  ，这样超平面为 $\sum\limits_{i=0}^{n}\theta_{i}x_{i} = 0$  进一步用向量来表示为：$\theta \bullet x = 0$  , 其中 $θ$ 为 $(n+1)\times1$ 的向量，$x$ 为 $(n+1)\times 1$ 的向量, ∙∙为内积，后面我们都用向量来表示超平面。

 

而感知机的模型可以定义为：$y = sign(\theta \bullet x)$  其中：


$$
sign(x)= \begin{cases} -1& {x<0}\\ 1& {x\geq 0} \end{cases}
$$


## **2. 感知机模型损失函数**

为了后面便于定义损失函数，我们将满足 $\theta \bullet x > 0$  的样本类别输出值取为1，满足 $\theta \bullet x < 0$ 的样本类别输出值取为-1，  这样取y的值有一个好处，就是方便定义损失函数。因为正确分类的样本满足 $y\theta \bullet x > 0$ ，而错误分类的样本满足 $y\theta \bullet x < 0$  , 我们损失函数的优化目标，就是期望使误分类的所有样本，到超平面的距离之和最小。

由于 $y\theta \bullet x < 0$  ，所以对于每一个误分类的样本 $i $ ，到超平面的距离是


$$
- y^{(i)}\theta \bullet x^{(i)}\big / \|\theta\|_2
$$


其中 $ \|\|\theta\|\|_2$ 为L2范数。

我们假设所有误分类的点的集合为M，则所有误分类的样本到超平面的距离之和为：


$$
- \sum\limits_{x_i \in M}y^{(i)}\theta \bullet x^{(i)}\big / ||\theta||_2
$$


这样我们就得到了初步的感知机模型的损失函数。

我们研究可以发现，分子和分母都含有 $θ$ ,当分子的 $θ$ 扩大 N 倍时，分母的 L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，即最终感知机模型的损失函数简化为：


$$
J(\theta) = - \sum\limits_{x_i \in M}y^{(i)}\theta \bullet x^{(i)}
$$






## **3. 感知机模型损失函数的优化方法**

感知机的损失函数：$J(\theta) = - \sum\limits_{x_i \in M}y^{(i)}\theta \bullet x^{(i)}$  , 其中M是所有误分类的点的集合。这个损失函数可以用梯度下降法或者拟牛顿法来解决，常用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。**所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）**。如果对这几种梯度下降法的区别不了解，可以参考另一篇文章[梯度下降（Gradient Descent）小结](http://www.cnblogs.com/pinard/p/5970503.html)。

**感知机模型选择的是采用随机梯度下降，这意味着我们每次仅仅需要使用一个误分类的点来更新梯度。**

损失函数基于 $θ$ 向量的的偏导数为：


$$
\frac{\partial}{\partial \theta}J(\theta) = - \sum\limits_{x_i \in M}y^{(i)}x^{(i)}
$$


$θ$ 的梯度下降迭代公式应该为：


$$
\theta = \theta  + \alpha\sum\limits_{x_i \in M}y^{(i)}x^{(i)}
$$


由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第 $i$ 个样本来更新梯度，则简化后的 $θ$ 向量的梯度下降迭代公式为：


$$
\theta = \theta  + \alpha y^{(i)}x^{(i)}
$$


其中  $α$ 为步长，$y^{(i)}$  为样本输出 1 或者 -1，$x^{(i)}$ 为 $(n+1)\times 1$ 的向量。 



## **3. 感知机模型的算法**

算法的输入为 m 个样本，每个样本对应于 n 维特征和一个二元类别输出1或者-1，如下：


$$
(x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)
$$


输出为分离超平面的模型系数 $θ$ 向量

